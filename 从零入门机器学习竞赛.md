# Datawhale AI 夏令营-电力需求预测赛

学习者手册:https://exn8g66dnwu.feishu.cn/docx/T7WGd7goqowRvFxwoApclo9Pn0b

赛事地址(数据集下载):https://challenge.xfyun.cn/topic/info?type=electricity-demand&option=ssgy&ch=dw24_uGS8Gs

你可以在[power_predict_ipynb](https://github.com/machenme/Datawhale/blob/main/power_predict/main.ipynb)看到我的代码
<details>

<summary>task1:初步了解项目</summary>

## 数据类型

赛题数据由训练集和测试集组成，为了保证比赛的公平性，将每日日期进行脱敏，用1-N进行标识，即1为数据集最近一天，其中1-10为测试集数据。

|特征字段|	字段描述|
|--|--|
|id	|房屋id|
|dt	|日标识|
|type	|房屋类型|
|target	|实际电力消耗，预测目标|

## 完整代码:

```python
# 1. 导入需要用到的相关库
# 导入 pandas 库，用于数据处理和分析
import pandas as pd
# 导入 numpy 库，用于科学计算和多维数组操作
import numpy as np

# 2. 读取训练集和测试集
# 使用 read_csv() 函数从文件中读取训练集数据，文件名为 'train.csv'
train = pd.read_csv('./data/data283931/train.csv')
# 使用 read_csv() 函数从文件中读取测试集数据，文件名为 'train.csv'
test = pd.read_csv('./data/data283931/test.csv')

# 3. 计算训练数据最近11-20单位时间内对应id的目标均值
target_mean = train[train['dt']<=20].groupby(['id'])['target'].mean().reset_index()

# 4. 将target_mean作为测试集结果进行合并
test = test.merge(target_mean, on=['id'], how='left')

# 5. 保存结果文件到本地
test[['id','dt','target']].to_csv('submit.csv', index=None)
```

## 学习代码

```python
target_mean = train[train['dt']<=20].groupby(['id'])['target'].mean().reset_index()
```
- `train[train['dt']<=20]`读取`train`中`dt`列小于等于20的所有数据

- `groupby`函数将数据进行分组然后再进行下一步操作
    - 通过`groupby(['id'])`告诉系统以`id`这列数据进行分组,`id`相同的数据均会被分到一个组里.
    - `groupby(['id'])['target']`则是分组之后只需要`target`这列数据
    - `groupby(['id'])['target'].mean()`获取每个分组的`target`的平均值
- `reset_index()`重建数据索引

```python
test = test.merge(target_mean, on=['id'], how='left')
```
`merge`函数用来合并两个DateFrame
- `df1.merge(df2)`与`pd.merge(df1,df2)`是等价的,都是合并`df1`与`df2`数据
- `on`以哪一列作为合并的依据,这里以`id`列作为合并的依据
- `how`如何合并,`left`保留左侧df1的所有行，如果右侧df2中没有匹配的键，则相应的列将填充为 NaN
    - `right`以右侧为准,如果左侧没有对应的数据填充NaN
    - `inner`取交集,合并后的数据只有左右两个df都有的部分
    - `outer`取并集,保留二者所有行,没有的部分填充NaN

显然我们不能够简单的用过去11天到20天的平均值作为过去1到10天的预测依据.我们应该找到更好的预测手段.

</details>

<details>

<summary>Task2:特征工程入门</summary>

随着昨天运行了baseline之后,出现了新的问题,对于本次数据是否存在一些规律性,例如按照某些时间间隔出现周期性重复?比如常见的7天,30天,90天等等.数据只有500个日期左右,大于90天的周期性也许意义不大了.

或者某些特征会对最终的预测有着更大的影响,而某些特征可能对实际预测结果基本没有意义甚至是负面影响?
```python
# 先查看数据是否存在规律
import matplotlib.pyplot as plt

plt.rcParams["font.sans-serif"] = ["SimHei"]  # 用来正常显示中文标签
plt.rcParams["axes.unicode_minus"] = False  # 用来正常显示负号

def draw_pic(data):
    unique_ids = data["id"].unique()
    plt.figure(figsize=(12,6))

    for id in unique_ids[:5]:
        demo_data = data[data["id"] == id]
        # 为了更直观理解图像,将target翻转,题目含义是距离当天的时间,那么反过来就是历史时间,
        # 例如0就会对应第一天,1对应第二天,一直到496天,然后通过496天数据来预测未来10天数据.
        plt.plot(demo_data["target"].tolist()[::-1], label=f"ID: {id}")
    plt.legend()
    plt.title('不同ID对应的每日用电量')
    plt.xlabel('日期')
    plt.ylabel('每日用电量')
    plt.show()

draw_pic(train)
```
![任意5个用户](https://github.com/machenme/Datawhale/blob/main/power_predict/imgs/output.png)

```python
# 也许不同的type类型也有影响?
same_type_train = train[train['type']==0]
draw_pic(same_type_train)
```
![相同Type下任意5个用户](https://github.com/machenme/Datawhale/blob/main/power_predict/imgs/output_same_id.png)



查看不同Type对应的平均用电量
![不同Type对应的平均用电量](https://github.com/machenme/Datawhale/blob/main/power_predict/imgs/diff_type_power_usage.png)



## LightGBM
- LightGBM 是一个梯度 boosting 框架, 使用基于学习算法的决策树. 它是分布式的, 高效的, 装逼的, 它具有以下优势: **速度和内存使用的优化** **减少分割增益的计算量** **通过直方图的相减来进行进一步的加速** **减少内存的使用** **减少并行学习的通信代价** ... 反正就是很多优点
- 项目地址: https://lightgbm.cn/

## 如何使用LightGBM
### 导入对应的软件包
```python
    # 直接上工具LightGBM
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.metrics import (
    mean_squared_log_error,
    mean_absolute_error,
    mean_squared_error,
)
import tqdm
import sys
import os
import gc
import argparse
import warnings

warnings.filterwarnings("ignore")
```
- 导入对应的数据集
```python
train = pd.read_csv("./dataset/train.csv")
test = pd.read_csv("./dataset/test.csv")
```
### 特征工程
```python
# 合并训练数据和测试数据，并进行排序
data = pd.concat([test, train], axis=0, ignore_index=True)
data = data.sort_values(['id','dt'], ascending=False).reset_index(drop=True)

# 历史平移
for i in range(10,30):
    data[f'last{i}_target'] = data.groupby(['id'])['target'].shift(i)
    
# 窗口统计
data[f'win3_mean_target'] = (data['last10_target'] + data['last11_target'] + data['last12_target']) / 3

# 进行数据切分
train = data[data.target.notnull()].reset_index(drop=True)
test = data[data.target.isnull()].reset_index(drop=True)

# 确定输入特征
train_cols = [f for f in data.columns if f not in ['id','target']]
```
- `pd.concat([test, train], axis=0, ignore_index=True)`
    - `pd.concat` 堆叠数据,与`merge`不同,不需要任何依据
    - `axis=0, ignore_index=True` 按照行合并.`axis=1`则是按照列合并.`ignore_index=True`则是忽略原来的索引
- `sort_values(['id','dt'], ascending=False).reset_index(drop=True)`
    - `sort_values` 对`DateFrame`数据进行排序,后面跟排序依据
    - `['id','dt'], ascending=False` 以 `id`作为第一排序依据,如果`id`相同,再用`dt`作为第二排序依据,并且`ascending=False`说明不需要反转,那么默认就是从大到小排序,也就是降序排列.
    - `reset_index(drop=True)` 排序后重建`DateFrame`索引并且舍弃原来的索引.

```python
for i in range(10,30):
    data[f'last{i}_target'] = data.groupby(['id'])['target'].shift(i)
```
- `shift(i)`将数据下移`i`行,前面不足的部分用`NaN`填充后新建一列存在数据的最后

|id|	dt	|type	|target	|last10_target	|last11_target|	last12_target|	last13_target	|last14_target|	last15_target|	
|--|--|--|--|--|--|--|--|--|--|
|fff81139a7|	496|	5|	23.288|	18.145|	NaN|	NaN|	NaN|	NaN	|NaN|
|fff81139a7	|495|	5|	25.252|	22.021	|18.145|	NaN|	NaN	|NaN|	NaN
|fff81139a7	|494|	5|	16.963|	21.282|	22.021|	18.145	|NaN|	NaN|	NaN|
|ff81139a7|	493|	5|	29.759|	22.818|	21.282|	22.021|	18.145|NaN|NaN|

```python
data[f'win3_mean_target'] = (data['last10_target'] + data['last11_target'] + data['last12_target']) / 3
```
- 将过去三天(今天的前第10天,第11天,第12天)的数据求平均值

```python
# 进行数据切分
train = data[data.target.notnull()].reset_index(drop=True)
test = data[data.target.isnull()].reset_index(drop=True)
```
- `notnull()` 只保留非空行的数据

注意的训练集和验证集的构建：因为数据存在时序关系，所以需要严格按照时序进行切分，并且时间序列问题只能是过去的事情对未来造成影响,反过来则没有意义

### 创建模型
```python
def time_model(lgb, train_df, test_df, cols):
    # 训练集和验证集切分
    trn_x, trn_y = train_df[train_df.dt>=31][cols], train_df[train_df.dt>=31]['target']
    val_x, val_y = train_df[train_df.dt<=30][cols], train_df[train_df.dt<=30]['target']
    # 构建模型输入数据
    train_matrix = lgb.Dataset(trn_x, label=trn_y)
    valid_matrix = lgb.Dataset(val_x, label=val_y)
    # lightgbm参数
    lgb_params = {
        'boosting_type': 'gbdt',
        'objective': 'regression',
        'metric': 'mse',
        'min_child_weight': 5,
        'num_leaves': 2 ** 5,
        'lambda_l2': 10,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 4,
        'learning_rate': 0.05,
        'seed': 2024,
        'nthread' : 16,
        'verbose' : -1,
    }
    # 训练模型
    model = lgb.train(lgb_params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], 
                      categorical_feature=[], verbose_eval=500, early_stopping_rounds=500)
    # 验证集和测试集结果预测
    val_pred = model.predict(val_x, num_iteration=model.best_iteration)
    test_pred = model.predict(test_df[cols], num_iteration=model.best_iteration)
    # 离线分数评估
    score = mean_squared_error(val_pred, val_y)
    print(score)
       
    return val_pred, test_pred
    
lgb_oof, lgb_test = time_model(lgb, train, test, train_cols)

# 保存结果文件到本地
test['target'] = lgb_test
test[['id','dt','target']].to_csv('submit.csv', index=None)
```
- LightGBM 可以直接使用 categorical features(分类特征)作为 input(输入). 它不需要被转换成 `one-hot coding(独热编码)`, 并且它比独热编码更快(约快上 8 倍)
    - 在构造 `Dataset` 之前, 应该将分类特征转换为 `int` 类型的值.

自己添加了L1正则化优化,结果发现不能乱改,改了反而降低了准确率QaQ.后面希望有更准确的做法

更新:当回溯周期提高到90天后,获得了稍好的成绩,哈哈哈哈



</details>