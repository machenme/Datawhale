### 什么是机器学习

机器学习（ML）和深度学习（DL）是使机器具备“学习”能力的技术。机器学习的核心目标是通过数据找出一个函数。这个函数可以帮助完成许多任务，如语音识别、图像识别和围棋决策等。

- **回归（Regression）**：如果机器学习任务的目标是预测一个数值，如预测未来的PM2.5数值，这类任务称为回归。回归的输出是一个标量，机器需要根据输入数据预测出一个数值结果。
- **分类（Classification）**：分类任务让机器从预设的多个类别中选择一个输出。例如垃圾邮件检测，机器会判断一封邮件是否为垃圾邮件，这是一种二分类任务。分类可以包含两个或多个类别。

### 简单案例

根据过往日期播放量预测未来日期播放量



### 根据过往日期播放量预测未来日期播放量的机器学习过程


**第一步**

- 首先，确定需要预测的数据，即当天的观看人数 $y$。
- 选取与预测相关的特征，这里我们使用前一天的观看人数 $x_1$ 作为输入特征。
- 我们假设观看人数 $y$ 与前一天的观看人数 $x_1$ 存在某种线性关系。最简单的预测模型可以写成一个带有未知参数的函数：
  
  $$
  y = w \cdot x_1 + b
  $$

  其中：
  - $y$ 是目标变量，表示预测的观看人数。
  - $x_1$ 是特征变量，即前一天的观看人数。
  - $w$ 是**权重**（weight），表示特征对预测结果的影响。
  - $b$ 是**偏置**（bias），用于调整预测结果。

- **模型**（model）是带有未知参数的函数，这些参数（$w$ 和 $b$）需要通过数据来估计。


---

**第二步：定义损失函数**

- **损失函数**（Loss Function）用于衡量模型预测值与真实值之间的差距，损失函数的输入是模型中的未知参数 $w$ 和 $b$。该函数可以表示为 $L(w, b)$。
  
- 假设模型为 $y = b + w ∗ x_1$，其中 $y$ 为预测值，$x_1$ 为前一天的观看次数。$b$ 和 $w$ 是未知参数，需通过训练数据进行优化。

- 利用训练数据计算预测值 $ŷ$ 和真实值 $y$ 之间的误差 $e$，例如：

  - 假设 $b = 500$ 和 $w = 1$，并代入前一天的观看次数 $x_1 = 4800$，得到预测值 $ŷ = 5300$。如果真实的观看次数 $y = 4900$，误差 $e1$ 计算为：
  
    $$
    e1 = |y - ŷ| = |4900 - 5300| = 400
    $$

  - 类似地，使用后续数据可以继续计算误差，例如 $e2 = 2100$。

- 将所有训练数据的误差累加并取平均，得到总损失 $L$：

  $$
  L = \frac{1}{N} \sum_{n=1}^{N} e_n
  $$
  其中 $N$ 为训练数据的总数量，$e_n$ 为每一天的误差。

- **损失函数越小，模型的预测效果越好**。通过调整 $w$ 和 $b$，可以找到使得损失最小的参数组合。

### 常用损失函数类型

1. **平均绝对误差**（MAE）：
   - 计算预测值与真实值的绝对差距。
   $
   e = |ŷ - y|
   $

2. **均方误差**（MSE）：
   - 计算预测值与真实值的平方差距，放大较大的误差。
   $
   e = (ŷ - y)^2
   $

3. **交叉熵**（Cross Entropy）：
   - 用于概率分布任务中，衡量两个概率分布之间的差异。

### 等高线图与误差表面

- 通过不同的参数 $w$ 和 $b$ 组合，可以计算出对应的损失值，并绘制等高线图。等高线图展示了不同参数组合下的损失大小，形成了所谓的 **误差表面**（Error Surface）。

- 在图中，**红色区域表示损失较大**，说明参数组合不理想；**蓝色区域表示损失较小**，说明参数组合较好。

  - 例如，$w = -0.25$ 和 $b = -500$ 可能表示观看人数逐渐减少，损失较大。
  - 而 $w = 0.75$ 和 $b = 500$ 则可能会给出较为准确的预测。



---


### 第三步：解最优化问题

在机器学习的第三步，我们需要找到一组最佳的参数 $w*$ 和 $b*$，使得损失函数 $L$ 达到最小值。这一过程可以通过 **梯度下降（Gradient Descent）** 等优化算法来实现。

#### 梯度下降的步骤：
1. **随机初始化参数**：
   - 选择一个随机的初始点 $w0$。

2. **计算损失函数的导数**：
   - 计算损失函数 $L(w)$ 关于参数 $w$ 的导数，即 $\frac{\partial L}{\partial w}$，这是误差表面的切线斜率。
  
3. **根据斜率更新参数**：
   - 如果斜率是负的，表示当前点的左边比右边高，此时应当增大 $w$ 的值，使得损失变小。
   - 如果斜率是正的，表示当前点的右边比左边高，应当减小 $w$ 的值，使得损失变小。

4. **学习率**（Learning Rate，记作 $η$）：
   - 学习率决定了每次参数更新的步伐大小。学习率越大，每次更新的幅度越大，模型学习得越快；学习率越小，更新幅度越小，学习过程越慢。
   - 学习率是一个**超参数**，需要由我们提前设定，不是通过训练数据自动学习的。

#### 梯度下降过程：
- 梯度下降的思想类似于一个人站在山坡上，想要走到最低点。他左右环视，计算斜率（即微分），如果右边更低，他就向右走；如果左边更低，他就向左走。步伐的大小由斜率和学习率共同决定。
  
- 在每一步的更新过程中，最终的目标是找到一个 $w$ 使得损失函数 $L(w)$ 达到最小值，这时的参数 $w*$ 被称为最优参数。

#### 问题中的示例：
- 在我们的例子中，假设损失函数曲线呈现不同的形状，可能损失函数并不总是正值，而是由定义的方式决定。例如，如果定义的损失函数为“绝对误差再减去100”，那么损失函数的值可能会变为负数。

### 梯度下降公式：
梯度下降的参数更新公式如下：
$$w_{new} = w_{old} - \eta \frac{\partial L}{\partial w} |_{w=w_{old}}$$
- 其中，$η$ 是学习率，$w_{old}$ 是当前参数值，$w_{new}$ 是更新后的参数值。

通过不断迭代这个公式，我们可以逐步逼近损失函数的最小值，找到最优参数组合。

### 梯度下降与误差表面：
在误差表面上，不同参数组合形成不同的损失值，梯度下降算法通过沿着损失最小的方向（梯度的反方向）不断更新参数，直到找到全局或局部最优解。

